import pandas as pd
import numpy as np
from sklearn.ensemble import IsolationForest

# Sample data with valid negative prices
data = {
    'trade_id': [1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2],
    'date': pd.date_range('2024-01-01', periods=14, freq='D'),
    'price': [10, -5, 30, 100, 40, -50, 45, 100, 200, 300, -1000, 400, 500, 450]  # Valid negative prices
}

df = pd.DataFrame(data)

# Preprocess data
df['date'] = pd.to_datetime(df['date'])

# Define a function to detect anomalies for each trade ID separately
def detect_anomalies(group):
    # Apply rolling statistics on the 'price' feature for the past 5 days
    group['rolling_mean'] = group['price'].rolling(5, min_periods=1).mean()
    group['rolling_std'] = group['price'].rolling(5, min_periods=1).std()
    
    # Drop NaN values resulting from rolling operations
    group = group.dropna(subset=['rolling_mean', 'rolling_std'])
    
    # Check if the group has enough rows to apply the model
    if group.shape[0] > 0:  # Ensure that the group is not empty
        # Fit the Isolation Forest model to detect anomalies based on 'price' and 'rolling' features
        features = ['price', 'rolling_mean', 'rolling_std']
        
        # Fit the model if there's enough data
        if group[features].shape[0] > 0:
            clf = IsolationForest(contamination=0.05, random_state=42)
            group['anomaly_score'] = clf.fit_predict(group[features])
        else:
            group['anomaly_score'] = np.nan  # Handle case with no valid rows
    else:
        group['anomaly_score'] = np.nan  # Skip empty groups

    return group

# Group by 'trade_id' and apply the function to each group
df_anomaly_detected = df.groupby('trade_id').apply(detect_anomalies)

# Anomalies
anomalies = df_anomaly_detected[df_anomaly_detected['anomaly_score'] == -1]

# Visualize results
import matplotlib.pyplot as plt

# Plot all data
plt.scatter(df_anomaly_detected['date'], df_anomaly_detected['price'], color='blue', label='Normal')

# Highlight anomalies
plt.scatter(anomalies['date'], anomalies['price'], color='red', label='Anomaly')
plt.legend()
plt.show()

# Print anomalies
print(anomalies[['trade_id', 'date', 'price', 'anomaly_score']])
import numpy as np
import pandas as pd
from sklearn.cluster import DBSCAN
from sklearn.preprocessing import StandardScaler

# Function to apply DBSCAN in batches
def apply_dbscan_in_batches(X, batch_size, eps=0.5, min_samples=5):
    dbscan = DBSCAN(eps=eps, min_samples=min_samples)
    
    # Initialize lists to store labels and indices
    all_labels = []
    all_indices = []

    # Process data in batches
    for i in range(0, len(X), batch_size):
        X_batch = X[i:i + batch_size]  # Select batch
        indices_batch = X.index[i:i + batch_size]  # Get the indices of the batch

        # Apply DBSCAN on the batch
        labels_batch = dbscan.fit_predict(X_batch)
        
        # Append the labels and indices
        all_labels.extend(labels_batch)
        all_indices.extend(indices_batch)
    
    # Combine all the labels and indices into a DataFrame
    return pd.DataFrame({'index': all_indices, 'labels': all_labels})

# Example usage:
# Load your data (X should be a DataFrame or NumPy array of numerical data)
X = df.select_dtypes(include=['float32', 'float64', 'int32', 'int64'])  # Select numerical columns

# Standardize the data to improve DBSCAN performance
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Convert scaled data back to DataFrame with original indices
X_scaled_df = pd.DataFrame(X_scaled, index=df.index)

# Set batch size (choose a size that fits in memory)
batch_size = 10000

# Apply DBSCAN in batches
dbscan_results = apply_dbscan_in_batches(X_scaled_df, batch_size, eps=0.5, min_samples=5)

# Merge the results back with the original DataFrame
df_with_labels = df.merge(dbscan_results, left_index=True, right_on='index', how='left')

# Now `df_with_labels` contains the original data and the DBSCAN labels
import pandas as pd
import numpy as np
from sklearn.ensemble import IsolationForest

# Assume `df` is your trading data with a `trade_id` and `date` column
df['date'] = pd.to_datetime(df['date'])

# Create rolling features (e.g., 5-day rolling mean)
df['rolling_mean'] = df.groupby('trade_id')['numerical_feature'].rolling(5).mean().reset_index(0, drop=True)

# Isolation Forest for Anomaly Detection
clf = IsolationForest(contamination=0.01)
features = ['numerical_feature1', 'numerical_feature2', 'rolling_mean']
df['anomaly_score'] = clf.fit_predict(df[features])

# Anomalies
df_anomalies = df[df['anomaly_score'] == -1]

# Visualize Anomalies
import matplotlib.pyplot as plt
plt.scatter(df['date'], df['numerical_feature'], color='blue', label='Normal')
plt.scatter(df_anomalies['date'], df_anomalies['numerical_feature'], color='red', label='Anomaly')
plt.legend()
plt.show()

import pandas as pd
import numpy as np
from sklearn.ensemble import IsolationForest

# Sample data
data = {
    'trade_id': [1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2],
    'date': pd.date_range('2024-01-01', periods=14, freq='D'),
    'price': [10, 20, 30, 100, 40, 50, 45, 100, 200, 300, 7000, 400, 500, 450]
}

df = pd.DataFrame(data)

# Preprocess data
df['date'] = pd.to_datetime(df['date'])

# Define a function to detect anomalies for each trade ID separately
def detect_anomalies(group):
    # Apply rolling statistics on the 'price' feature for the past 5 days
    group['rolling_mean'] = group['price'].rolling(5, min_periods=1).mean()
    group['rolling_std'] = group['price'].rolling(5, min_periods=1).std()
    
    # Fit the Isolation Forest model to detect anomalies based on 'price' and 'rolling' features
    features = ['price', 'rolling_mean', 'rolling_std']
    clf = IsolationForest(contamination=0.05, random_state=42)
    
    # Drop NaN values that may come from rolling operations
    group = group.dropna(subset=features)
    
    # Predict anomaly scores (-1 means anomaly, 1 means normal)
    group['anomaly_score'] = clf.fit_predict(group[features])
    
    return group

# Group by 'trade_id' and apply the function to each group
df_anomaly_detected = df.groupby('trade_id').apply(detect_anomalies)

# Anomalies
anomalies = df_anomaly_detected[df_anomaly_detected['anomaly_score'] == -1]

# Visualize results
import matplotlib.pyplot as plt

# Plot all data
plt.scatter(df_anomaly_detected['date'], df_anomaly_detected['price'], color='blue', label='Normal')

# Highlight anomalies
plt.scatter(anomalies['date'], anomalies['price'], color='red', label='Anomaly')
plt.legend()
plt.show()

# Print anomalies
print(anomalies[['trade_id', 'date', 'price', 'anomaly_score']])



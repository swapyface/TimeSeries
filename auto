import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder, StandardScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import Adam
from tensorflow.keras import Input
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
import keras_tuner as kt

# Assuming your clean dataset is a Pandas DataFrame with 'category' and 'numerical_column'
df = pd.DataFrame({
    'category': ['A', 'A', 'B', 'B', 'C', 'C'],
    'numerical_column': [100, 110, 200, 195, 305, 315]
})

# Step 1: Prepare the data
# Label encode the categorical column
le = LabelEncoder()
df['category_encoded'] = le.fit_transform(df['category'])

# Standardize the numerical column
scaler = StandardScaler()
df['scaled_numerical'] = scaler.fit_transform(df[['numerical_column']])

# Step 2: Define a model-building function for hyperparameter tuning
def build_autoencoder(hp):
    model = Sequential([
        Input(shape=(1,)),
        Dense(units=hp.Int('units1', min_value=8, max_value=64, step=8), activation='relu'),
        Dense(units=hp.Int('units2', min_value=4, max_value=32, step=4), activation='relu'),
        Dense(units=hp.Int('units1', min_value=8, max_value=64, step=8), activation='relu'),
        Dense(1, activation='linear')
    ])
    
    # Tune the learning rate for the Adam optimizer
    learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])
    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mse')
    
    return model

# Step 3: Train and tune the model for each category separately
categories = df['category'].unique()

for category in categories:
    print(f"Training and tuning autoencoder for category: {category}")
    
    # Select the data for the current category
    category_data = df[df['category'] == category]['scaled_numerical'].values
    category_data = category_data.reshape(-1, 1)  # Reshape for the model
    
    # Train/test split
    X_train, X_test = train_test_split(category_data, test_size=0.2, random_state=42)
    
    # Step 4: Hyperparameter tuning using Keras Tuner
    tuner = kt.RandomSearch(
        build_autoencoder,
        objective='val_loss',
        max_trials=5,  # You can increase this for more extensive tuning
        executions_per_trial=1,
        directory='autoencoder_tuning',
        project_name=f'category_{category}_autoencoder'
    )
    
    tuner.search(X_train, X_train, epochs=50, validation_data=(X_test, X_test), verbose=0)
    
    # Get the best model from the tuner
    best_model = tuner.get_best_models(num_models=1)[0]
    
    # Reconstruct the entire category data using the best model
    reconstruction = best_model.predict(category_data)
    
    # Inverse transform to get original values
    original_values = scaler.inverse_transform(category_data)
    reconstructed_values = scaler.inverse_transform(reconstruction)
    
    # Print original and reconstructed values
    print(f"Original and Reconstructed values for category '{category}':")
    for original, reconstructed in zip(original_values, reconstructed_values):
        print(f"Original: {original[0]}, Reconstructed: {reconstructed[0]}")
    
    # Step 5: Calculate and print the model score (MSE)
    mse = mean_squared_error(original_values, reconstructed_values)
    print(f"Model MSE for category '{category}': {mse}\n")